{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/digitalhumanitiestextbook/dhtextbook/blob/main/chapter09/9_topic_modeling_practice.ipynb\" target=\"_parent\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohVP8V0oq0AP",
        "outputId": "c27eb3f7-0afc-481a-aacd-7f57333fb426"
      },
      "outputs": [],
      "source": [
        "# 패키지 설치\n",
        "!pip uninstall numpy scipy gensim pandas -y\n",
        "!pip install numpy==1.26.4 -qq\n",
        "!pip install scipy==1.11.4 -qq\n",
        "!pip install pandas==2.2.2 -qq\n",
        "!pip install gensim==4.3.2 -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHpKlUMX4tXm",
        "outputId": "f8f3ebb0-0753-4498-ccee-ebde8468efd5"
      },
      "outputs": [],
      "source": [
        "# 세션 재실행\n",
        "import IPython\n",
        "IPython.Application.instance().kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bXR_Mcl64wlT",
        "outputId": "20215f7d-8074-47af-a3f4-533d6b9ed9bb"
      },
      "outputs": [],
      "source": [
        "# 패키지 설치\n",
        "!pip install pyLDAvis -qq\n",
        "!pip install -qq -U gensim\n",
        "!pip uninstall spacy thinc -y\n",
        "!pip install spacy==3.6.1 --no-deps -qq\n",
        "!pip install thinc==8.1.12 --no-deps -qq\n",
        "!pip install matplotlib -qq\n",
        "!pip install seaborn -qq\n",
        "!python -m spacy download en_core_web_md -qq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 아래 코드를 실행한 후 나타나는 \"파일 선택\" 버튼을 클릭하여, 분석할 txt 파일을 업로드하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# 파일 업로드\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 업로드한 파일 경로 가져오기\n",
        "file_path = list(uploaded.keys())[0]\n",
        "root, ext = os.path.splitext(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import pandas as pd\n",
        "\n",
        "# NLTK의 영어 문장 토큰화 도구 다운로드\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# 파일 읽기\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# 문장 단위 분리\n",
        "texts = sent_tokenize(text)\n",
        "\n",
        "# 데이터프레임 초기화\n",
        "data = pd.DataFrame({\n",
        "    \"number\": range(1, len(texts) + 1),\n",
        "    \"text\": texts\n",
        "})\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6tBq88cI7OU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import spacy\n",
        "import pyLDAvis.gensim_models\n",
        "import en_core_web_md\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models import LdaMulticore\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "sns.set()  # Seaborn 기본 스타일 적용\n",
        "pyLDAvis.enable_notebook()  # PyLDAvis가 주피터 노트북 내부에 바로 시각화되도록 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCqP2owEJWZF"
      },
      "outputs": [],
      "source": [
        "# spaCy 모델:\n",
        "nlp = en_core_web_md.load()\n",
        "\n",
        "# 글에서 제거하고 싶은 품사태그 입력 (부사(ADV), 대명사(PRON), 접속사(CCONJ), 구두점(PUNCT), 입자(PART), 한정사(DET), 전치사/후치사(ADP), 공백(SPACE), 수사(NUM), 기호(SYM), 동사(VERB), 형용사(ADJ))\n",
        "removal= ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE', 'NUM', 'SYM', 'VERB', 'ADJ']\n",
        "\n",
        "# 불용어 사용자 설정\n",
        "custom_stop_words = ['nan', 'nat', 'shall', 'oh', 'mrs', 'mr', 'ms', 's', 'yes', 'shd', 'cd', 'wh', 'wd'] # 사용자가 임의로 지정한 불용어 추가하기\n",
        "tokens = []\n",
        "\n",
        "# 'text'열을 스트링으로 전환하여 잠재적으로 누락될 수 있는 값 처리\n",
        "for text in nlp.pipe(data['text'].astype(str).fillna('')):\n",
        "   proj_tok = [token.lemma_.lower() for token in text\n",
        "               if token.pos_ not in removal and\n",
        "               not token.is_stop and\n",
        "               token.is_alpha and\n",
        "               token.lemma_.lower() not in custom_stop_words]\n",
        "   tokens.append(proj_tok)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "qcdW2R21JWm-",
        "outputId": "4ae7af29-87c6-490b-f80e-6c8d1546781b"
      },
      "outputs": [],
      "source": [
        "# 토큰화된 결과를 데이터프레임에 추가\n",
        "data['tokens'] = tokens\n",
        "data['tokens']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gV1ltCKRJnk_"
      },
      "outputs": [],
      "source": [
        "# 딕셔너리로 각 단어마다 고유 아이디 부여\n",
        "dictionary = Dictionary(data['tokens'])\n",
        "\n",
        "# 빈도 기준으로 드문 단어와 너무 흔한 단어 제거 \n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)\n",
        "\n",
        "# 코퍼스 생성: 각 문서를 (단어 ID, 단어 빈도) 리스트로 변환\n",
        "corpus = [dictionary.doc2bow(doc) for doc in data['tokens']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "GoRND7ioGf8k",
        "outputId": "300bf9f4-099a-4292-cc4f-96babed071b5"
      },
      "outputs": [],
      "source": [
        "#C_umass를 이용해서 일관성 점수 구하기\n",
        "topics = []  # 토픽 개수 기록\n",
        "score = []  # 각 토픽 개수별 Coherence Score 기록\n",
        "\n",
        "# 토픽 개수를 1부터 19까지 변화시키며 평가\n",
        "for i in range(1, 20):\n",
        "   # LDA 모델 학습\n",
        "   lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=10, num_topics=i, workers = 4, passes=15, random_state=0)\n",
        "   \n",
        "   # CoherenceModel로 토픽 일관성 평가\n",
        "   coh_model = CoherenceModel(model=lda_model, corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
        "\n",
        "   # 토픽 개수 및 점수 저장\n",
        "   topics.append(i)\n",
        "   score.append(coh_model.get_coherence())\n",
        "\n",
        "# 토픽 개수 vs Coherence Score 시각화\n",
        "plt.plot(topics, score)  # 토픽 개수에 따른 Coherence Score 곡선 그리기\n",
        "plt.xlabel('Number of Topics')  # x축 레이블 설정\n",
        "plt.ylabel('Coherence Score')  # y축 레이블 설정\n",
        "plt.show()  # 그래프 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "J95cjB0lGg9L",
        "outputId": "dd2d2e22-fe28-4176-9950-17adffb0617b"
      },
      "outputs": [],
      "source": [
        "#c_v를 사용해서 일관성 점수 구하기\n",
        "topics = []  # 토픽 개수 기록\n",
        "score = []  # 각 토픽 개수별 Coherence Score 기록\n",
        "\n",
        "# 토픽 개수를 1부터 19까지 변화시키며 평가\n",
        "for i in range(1, 20):\n",
        "   # LDA 모델 학습\n",
        "   lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=10, num_topics=i, workers = 4, passes=15, random_state=0)\n",
        "\n",
        "   # CoherenceModel로 토픽 일관성 평가\n",
        "   coh_model = CoherenceModel(model=lda_model, texts=data['tokens'], corpus=corpus, dictionary=dictionary, coherence='c_v')\n",
        "   \n",
        "   # 토픽 개수 및 점수 저장\n",
        "   topics.append(i)\n",
        "   score.append(coh_model.get_coherence())\n",
        "\n",
        "# 토픽 개수 vs Coherence Score 시각화\n",
        "plt.plot(topics, score)  # 토픽 개수에 따른 Coherence Score 곡선 그리기\n",
        "plt.xlabel('Number of Topics')  # x축 레이블 설정\n",
        "plt.ylabel('Coherence Score')  # y축 레이블 설정\n",
        "plt.show()  # 그래프 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2knbPtjPJvW-",
        "outputId": "31d12979-618f-4668-a924-719934984ed3"
      },
      "outputs": [],
      "source": [
        "# LDA 모델 학습\n",
        "lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=50, num_topics=10, random_state=0, passes=15)\n",
        "\n",
        "# 학습된 모델의 모든 토픽 출력\n",
        "lda_model.print_topics(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV1aE3Z5J7nY"
      },
      "outputs": [],
      "source": [
        "#시각화 생성\n",
        "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary, sort_topics=False)\n",
        "pyLDAvis.display(lda_display)\n",
        "\n",
        "# 시각화 저장\n",
        "pyLDAvis.save_html(lda_display, 'index.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Colab 환경에서 생성된 HTML 파일을 로컬로 다운로드\n",
        "from google.colab import files\n",
        "\n",
        "files.download(\"index.html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWcureolKe-Y"
      },
      "outputs": [],
      "source": [
        "def make_topictable(lda_model, corpus):\n",
        "    topic_table = pd.DataFrame()  # 결과를 저장할 빈 DataFrame 생성\n",
        "\n",
        "    # 각 문서별 토픽 분포 계산\n",
        "    for i, topic_dist in enumerate(lda_model[corpus]):\n",
        "        # ldamodel.per_word_topics가 True면 topic_list[0]을 fasle면 topic_list 전체를 할당\n",
        "        doc = topic_dist[0] if lda_model.per_word_topics else topic_dist\n",
        "\n",
        "        # 각 문서에 대해 가장 비중이 높은 토픽 찾기\n",
        "        if doc:  # 토픽 분포가 비어있지 않은 경우\n",
        "            top_topic, weight = max(doc, key=lambda x: x[1]) # (토픽 ID, 확률) 중 확률이 가장 높은 것\n",
        "            new_row = pd.DataFrame([pd.Series([int(top_topic), round(weight, 4), topic_dist])]) # 새 행 생성: [가장 높은 토픽 ID, 그 토픽의 비중, 전체 토픽 분포]\n",
        "            topic_table = pd.concat([topic_table, new_row], ignore_index=True) # 기존 topic_table에 행 추가\n",
        "\n",
        "    return topic_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "AKBmtuQaKrIf",
        "outputId": "361b2c1f-704a-49e2-ebb9-3351ef027af4"
      },
      "outputs": [],
      "source": [
        "topictable = make_topictable(lda_model, corpus)  # 토픽 테이블 생성\n",
        "topictable = topictable.reset_index()  # 문서 번호를 의미하는 열로 사용하기 위해 인덱스를 추가\n",
        "topictable.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']  # 각 열의 의미를 명확히 지정\n",
        "topictable[:1614]  # 상위 1614개 문서의 토픽 정보 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "topictable.to_csv(\"topictable.csv\", index=False)\n",
        "files.download('topictable.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
